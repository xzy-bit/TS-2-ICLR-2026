accelerate==1.8.1
datasets==3.6.0
deepspeed==0.17.1
entmax
evalplus @ git+https://github.com/evalplus/evalplus@dac1da40276aff8c965bdf351952aaf6bfeb3cc9
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl#sha256=ac7096c73cf5381e1e067abc58edcc510a882e8a58d4cb34cfe87801ecf1c817
huggingface-hub==0.33.0
nltk==3.9.1
numpy==1.26.4
torch==2.5.1
transformers==4.52.4
triton==3.1.0
vllm==0.7.1